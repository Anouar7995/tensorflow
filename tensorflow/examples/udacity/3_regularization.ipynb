{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (65536, 28, 28) (65536,)\n",
      "Test set (18000, 28, 28) (18000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_index:  [    95  91150  81556  97999 180334]\n",
      "sample labels: [9 0 1 3 3]\n",
      "sample_index:  [103511  38557 111127 116144 185466]\n",
      "sample labels: [8 7 4 5 4]\n",
      "sample_index:  [ 76426 121810 180011 129997  62529]\n",
      "sample labels: [1 4 6 2 8]\n",
      "sample_index:  [157128 145199 157026 193805  42873]\n",
      "sample labels: [3 2 6 1 8]\n",
      "sample_index:  [  1975  49305 104612   6184  37481]\n",
      "sample labels: [4 6 3 8 7]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVsAAAD/CAYAAABfNXWhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c459110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def visual_confirmation(train_dataset, train_labels):\n",
    "     fig, plt_axes_arr=plt.subplots(5, 5)\n",
    "     for i in range(5):\n",
    "         sample_index = np.random.choice(train_dataset.shape[0],5)\n",
    "         print(\"sample_index: \", sample_index)\n",
    "         print(\"sample labels:\", train_labels[sample_index])\n",
    "         for j,idx in enumerate(sample_index):\n",
    "             plt_axes_arr[i,j].imshow(train_dataset[idx],cmap='Greys')\n",
    "             plt_axes_arr[i,j].axis('off')\n",
    "     plt.show()\n",
    "visual_confirmation(train_dataset,train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (65536, 784) (65536, 10)\n",
      "Test set (18000, 784) (18000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 179000\n",
    "beta=0.005\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random valued following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  loss = loss + beta*tf.nn.l2_loss(weights, name=None)\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 30.614008\n",
      "Training accuracy: 12.6%\n",
      "Validation accuracy: 15.2%\n",
      "Loss at step 100: 10.623415\n",
      "Training accuracy: 71.8%\n",
      "Validation accuracy: 71.9%\n",
      "Loss at step 200: 6.312225\n",
      "Training accuracy: 75.3%\n",
      "Validation accuracy: 75.4%\n",
      "Loss at step 300: 3.876953\n",
      "Training accuracy: 77.5%\n",
      "Validation accuracy: 77.6%\n",
      "Loss at step 400: 2.495596\n",
      "Training accuracy: 79.3%\n",
      "Validation accuracy: 79.4%\n",
      "Loss at step 500: 1.715370\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 80.9%\n",
      "Loss at step 600: 1.273819\n",
      "Training accuracy: 81.8%\n",
      "Validation accuracy: 81.8%\n",
      "Loss at step 700: 1.022390\n",
      "Training accuracy: 82.5%\n",
      "Validation accuracy: 82.4%\n",
      "Loss at step 800: 0.878230\n",
      "Training accuracy: 82.8%\n",
      "Validation accuracy: 82.8%\n",
      "Test accuracy: 89.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "      # just to get that one numpy array. Note that it recomputes all its graph\n",
    "      # dependencies.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#With RELU\n",
    "batch_size = 512\n",
    "num_nodes=512\n",
    "beta=0.0007\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "    \n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "  weights_0 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_nodes],stddev=0.01))\n",
    "  biases_0 = tf.Variable(tf.constant(0.1,shape=[512]))\n",
    "  \n",
    "    \n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes, num_labels]))\n",
    "  biases_1 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits_train_0 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_0) + biases_0)\n",
    "  logits_train_1 = tf.matmul(logits_train_0, weights_1) + biases_1\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits_train_1, tf_train_labels))\n",
    "  loss = loss + beta*(tf.nn.l2_loss(weights_0, name=None) + tf.nn.l2_loss(weights_1, name=None))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits_train_1)\n",
    "\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul((tf.nn.relu(tf.matmul(tf_valid_dataset, weights_0) + biases_0)), weights_1) + biases_1)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul((tf.nn.relu(tf.matmul(tf_test_dataset, weights_0) + biases_0)), weights_1) + biases_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 5.811259\n",
      "Minibatch accuracy: 9.0%\n",
      "Validation accuracy: 40.6%\n",
      "Minibatch loss at step 500: 2.050146\n",
      "Minibatch accuracy: 85.7%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 1000: 1.521429\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 1500: 1.157782\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 2000: 1.103972\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 2500: 0.948086\n",
      "Minibatch accuracy: 91.6%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 3000: 0.883045\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 3500: 0.787535\n",
      "Minibatch accuracy: 91.6%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 4000: 0.745144\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 4500: 0.667024\n",
      "Minibatch accuracy: 93.6%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 5000: 0.669701\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 5500: 0.592200\n",
      "Minibatch accuracy: 92.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 6000: 0.568495\n",
      "Minibatch accuracy: 92.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 6500: 0.541153\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 7000: 0.480807\n",
      "Minibatch accuracy: 93.6%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 7500: 0.437081\n",
      "Minibatch accuracy: 95.5%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 8000: 0.481670\n",
      "Minibatch accuracy: 94.1%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 8500: 0.447042\n",
      "Minibatch accuracy: 93.6%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 9000: 0.487886\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 9500: 0.426195\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 10000: 0.477200\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 90.4%\n",
      "Test accuracy: 95.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10003\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#With RELU\n",
    "batch_size = 16\n",
    "num_nodes=16\n",
    "beta=0.0007\n",
    "train_subset = 16\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "    \n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "  weights_0 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_nodes],stddev=0.01))\n",
    "  biases_0 = tf.Variable(tf.constant(0.05,shape=[16]))\n",
    "  \n",
    "    \n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes, num_labels]))\n",
    "  biases_1 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits_train_0 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_0) + biases_0)\n",
    "  logits_train_1 = tf.matmul(logits_train_0, weights_1) + biases_1\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits_train_1, tf_train_labels))\n",
    "  loss = loss + beta*(tf.nn.l2_loss(weights_0, name=None) + tf.nn.l2_loss(weights_1, name=None))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits_train_1)\n",
    "\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul((tf.nn.relu(tf.matmul(tf_valid_dataset, weights_0) + biases_0)), weights_1) + biases_1)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul((tf.nn.relu(tf.matmul(tf_test_dataset, weights_0) + biases_0)), weights_1) + biases_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 10\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 5 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#With RELU\n",
    "batch_size = 512\n",
    "num_nodes=512\n",
    "\n",
    "train_subset = 200000\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "    \n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "  weights_0 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_nodes],stddev=0.001))\n",
    "  biases_0 = tf.Variable(tf.constant(0.1,shape=[512]))\n",
    "  \n",
    "    \n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes, num_labels]))\n",
    "  biases_1 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits_train_0 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_0) + biases_0)\n",
    "  logits_train_1 = tf.nn.dropout(tf.matmul(logits_train_0, weights_1) + biases_1,0.5)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits_train_1, tf_train_labels))\n",
    "  \n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits_train_1)\n",
    "\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul((tf.nn.relu(tf.matmul(tf_valid_dataset, weights_0) + biases_0)), weights_1) + biases_1)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul((tf.nn.relu(tf.matmul(tf_test_dataset, weights_0) + biases_0)), weights_1) + biases_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_steps = 20501\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 1000 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#With RELU\n",
    "batch_size = 1024\n",
    "num_nodes=1024\n",
    "\n",
    "train_subset = 179000\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "    \n",
    "    \n",
    "  weights_0 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_nodes],stddev=0.001))\n",
    "  biases_0 = tf.Variable(tf.constant(0.001,shape=[1024]))\n",
    "    \n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes, num_nodes],stddev=0.001))\n",
    "  biases_1 = tf.Variable(tf.constant(0.001,shape=[1024]))\n",
    "  \n",
    "    \n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes, num_labels]))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits_train_0 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_0) + biases_0)\n",
    "  logits_train_1 = tf.nn.dropout(tf.nn.relu(tf.matmul(logits_train_0, weights_1) + biases_1),0.5)\n",
    "  logits_train_2 = tf.nn.dropout(tf.matmul(logits_train_1, weights_2) + biases_2,0.5)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits_train_2, tf_train_labels))\n",
    "  \n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0, trainable=False)\n",
    "  starter_learning_rate = 0.01\n",
    "  learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           1000, 0.96, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits_train_2)\n",
    "\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_0) + biases_0), weights_1) + biases_1), weights_2) + biases_2)\n",
    "    \n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_0) + biases_0), weights_1) + biases_1), weights_2) + biases_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_steps = 20501\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#With RELU\n",
    "batch_size = 256\n",
    "num_nodes=1024\n",
    "\n",
    "train_subset = 179817\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "    \n",
    "    \n",
    "  weights_0 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_nodes],stddev=0.01))\n",
    "  biases_0 = tf.Variable(tf.constant(0.001,shape=[1024]))\n",
    "    \n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes, num_nodes],stddev=0.01))\n",
    "  biases_1 = tf.Variable(tf.constant(0.001,shape=[1024]))\n",
    "  \n",
    "    \n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes, num_labels]))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits_train_0 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_0) + biases_0)\n",
    "  logits_train_1 = tf.nn.dropout(tf.nn.relu(tf.matmul(logits_train_0, weights_1) + biases_1),0.5)\n",
    "  logits_train_2 = tf.nn.dropout(tf.matmul(logits_train_1, weights_2) + biases_2,0.5)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits_train_2, tf_train_labels))\n",
    "  \n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0, trainable=False)\n",
    "  starter_learning_rate = 0.01\n",
    "  learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           500, 0.96, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "  #optimizer=tf.train.AdamOptimizer().minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits_train_2)\n",
    "\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_0) + biases_0), weights_1) + biases_1), weights_2) + biases_2)\n",
    "    \n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_0) + biases_0), weights_1) + biases_1), weights_2) + biases_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 10500\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#With RELU\n",
    "batch_size = 512\n",
    "num_nodes=1024\n",
    "beta=0.0007\n",
    "image_size=28\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "    \n",
    "  weights_0 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, 1024],stddev=0.01))\n",
    "  biases_0 = tf.Variable(tf.constant(0.07,shape=[1024]))\n",
    "  \n",
    "  \n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([1024, 512],stddev=0.01))\n",
    "  biases_1 = tf.Variable(tf.constant(0.07,shape=[512]))\n",
    "  \n",
    "\n",
    " \n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([512, 256],stddev=0.01))\n",
    "  biases_2 = tf.Variable(tf.constant(0.07,shape=[256]))\n",
    "\n",
    "\n",
    "  weights_3 = tf.Variable(\n",
    "    tf.truncated_normal([256, 256],stddev=0.01))\n",
    "  biases_3 = tf.Variable(tf.constant(0.1,shape=[256]))\n",
    "    \n",
    "    \n",
    "  weights_4 = tf.Variable(\n",
    "    tf.truncated_normal([256, num_labels]))\n",
    "  biases_4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits_train_0 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_0) + biases_0)\n",
    "  logits_train_1 = tf.nn.relu(tf.matmul(logits_train_0, weights_1) + biases_1)\n",
    "  logits_train_2 = tf.nn.relu(tf.matmul(logits_train_1, weights_2) + biases_2)\n",
    "  logits_train_3 = tf.nn.relu(tf.matmul(logits_train_2, weights_3) + biases_3)\n",
    "  logits_train_4 = tf.matmul(logits_train_3, weights_4) + biases_4\n",
    "\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits_train_4, tf_train_labels))\n",
    "  loss = loss + beta*(tf.nn.l2_loss(weights_0, name=None) + tf.nn.l2_loss(weights_1, name=None) \n",
    "                      + tf.nn.l2_loss(weights_2, name=None) + tf.nn.l2_loss(weights_3, name=None)\n",
    "                     + tf.nn.l2_loss(weights_4, name=None))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.025).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits_train_4)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_0) + biases_0), weights_1) + biases_1), weights_2) + biases_2),weights_3) + biases_3), weights_4) + biases_4)\n",
    "    \n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_0) + biases_0), weights_1) + biases_1), weights_2) + biases_2),weights_3) + biases_3), weights_4) + biases_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.428053\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 8.4%\n",
      "Minibatch loss at step 500: 1.358825\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 1000: 1.216680\n",
      "Minibatch accuracy: 84.0%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 1500: 1.164320\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 2000: 1.095818\n",
      "Minibatch accuracy: 87.9%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 2500: 1.015997\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 3000: 1.013274\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 3500: 0.982419\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 4000: 1.019284\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 4500: 0.961202\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 5000: 0.948277\n",
      "Minibatch accuracy: 92.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 5500: 0.905339\n",
      "Minibatch accuracy: 92.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 6000: 0.889545\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 6500: 0.878083\n",
      "Minibatch accuracy: 92.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 7000: 0.804363\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 7500: 0.728928\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 8000: 0.771650\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 8500: 0.781787\n",
      "Minibatch accuracy: 93.2%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 9000: 0.801285\n",
      "Minibatch accuracy: 92.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 9500: 0.766480\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 10000: 0.808063\n",
      "Minibatch accuracy: 92.0%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 10500: 0.671557\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 11000: 0.713689\n",
      "Minibatch accuracy: 94.9%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 11500: 0.670059\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 12000: 0.636641\n",
      "Minibatch accuracy: 95.9%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 12500: 0.798588\n",
      "Minibatch accuracy: 90.4%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 13000: 0.627769\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 13500: 0.586593\n",
      "Minibatch accuracy: 98.0%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 14000: 0.577105\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 14500: 0.768227\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 15000: 0.566139\n",
      "Minibatch accuracy: 97.5%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 15500: 0.610098\n",
      "Minibatch accuracy: 96.5%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 16000: 0.526314\n",
      "Minibatch accuracy: 99.4%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 16500: 0.602334\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 17000: 0.580834\n",
      "Minibatch accuracy: 97.3%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 17500: 0.540346\n",
      "Minibatch accuracy: 98.0%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 18000: 0.492959\n",
      "Minibatch accuracy: 99.0%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 18500: 0.502749\n",
      "Minibatch accuracy: 98.2%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 19000: 0.496439\n",
      "Minibatch accuracy: 98.6%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 19500: 0.485224\n",
      "Minibatch accuracy: 98.6%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 20000: 0.498830\n",
      "Minibatch accuracy: 97.9%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 20500: 0.489290\n",
      "Minibatch accuracy: 97.5%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 21000: 0.461268\n",
      "Minibatch accuracy: 99.4%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 21500: 0.460947\n",
      "Minibatch accuracy: 99.0%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 22000: 0.465273\n",
      "Minibatch accuracy: 98.6%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 22500: 0.457531\n",
      "Minibatch accuracy: 98.2%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 23000: 0.429732\n",
      "Minibatch accuracy: 99.4%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 23500: 0.440446\n",
      "Minibatch accuracy: 98.6%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 24000: 0.425590\n",
      "Minibatch accuracy: 99.0%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 24500: 0.412178\n",
      "Minibatch accuracy: 99.4%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 25000: 0.442520\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 25500: 0.401382\n",
      "Minibatch accuracy: 99.4%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 26000: 0.427212\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 26500: 0.398460\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 27000: 0.374799\n",
      "Minibatch accuracy: 99.6%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 27500: 0.378186\n",
      "Minibatch accuracy: 99.0%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 28000: 0.379059\n",
      "Minibatch accuracy: 99.6%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 28500: 0.382144\n",
      "Minibatch accuracy: 99.0%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 29000: 0.371445\n",
      "Minibatch accuracy: 99.6%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 29500: 0.367689\n",
      "Minibatch accuracy: 99.4%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 30000: 0.356337\n",
      "Minibatch accuracy: 99.8%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 30500: 0.347424\n",
      "Minibatch accuracy: 99.6%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 31000: 0.341354\n",
      "Minibatch accuracy: 99.6%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 31500: 0.339688\n",
      "Minibatch accuracy: 99.6%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 32000: 0.340799\n",
      "Minibatch accuracy: 99.6%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 32500: 0.336303\n",
      "Minibatch accuracy: 99.6%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 33000: 0.333679\n",
      "Minibatch accuracy: 99.4%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 33500: 0.330325\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 34000: 0.331311\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 34500: 0.327185\n",
      "Minibatch accuracy: 99.6%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 35000: 0.313700\n",
      "Minibatch accuracy: 99.8%\n",
      "Validation accuracy: 91.4%\n",
      "Test accuracy: 95.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 35001\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
